syntax = "proto3";

package lbann_data;

message LbannPB {
  DataReader data_reader = 1;
  Model model = 2;
  Optimizer optimizer = 3;
  MotifDefinitions motif_definitions = 4;
}

//========================================================================
// DataReaders
//========================================================================
message DataReader {
  int64 max_par_io_size = 1;
  repeated Reader reader = 2;
}

message Reader {
  string name = 1; //mnist, nci, nci_regression, numpy, imagenet, synthetic, merge_samples
  string role = 3; //train, validation, test
  bool shuffle = 4;
  string data_filedir = 5;
  string data_local_filedir = 50; //to support data_store
  string data_filename = 6;
  string label_filename = 7;
  double validation_percent = 9;
  int64 absolute_sample_count = 11;
  int64 first_n = 200;
  double percent_of_data_to_use = 12;
  //for GAN model
  bool gan_labelling = 201;
  int32 gan_label_value = 202;
  ImagePreprocessor image_preprocessor = 13;

  //------------------ start of only for jag_conduit -----------------------
  message JagLinearNormalizationParams {
    double scale = 1;
    double bias = 2;
  }

  repeated JagLinearNormalizationParams jag_image_normalization_params = 86;
  repeated JagLinearNormalizationParams jag_scalar_normalization_params = 87;
  repeated JagLinearNormalizationParams jag_input_normalization_params = 88;

  bool split_jag_image_channels = 89;
  repeated string jag_image_keys = 90;
  repeated string jag_scalar_keys = 91;
  repeated string jag_input_keys = 92;
  message JagKeyPrefixFilter {
    string key_prefix = 1;
    uint32 min_len = 2;
  }
  repeated string jag_scalar_filters = 93;
  repeated JagKeyPrefixFilter jag_scalar_prefix_filters = 94;
  repeated string jag_input_filters = 95;
  repeated JagKeyPrefixFilter jag_input_prefix_filters = 96;

  enum JAG_Data {
    Undefined  = 0;
    JAG_Image  = 1;
    JAG_Scalar = 2;
    JAG_Input  = 3;
  }
  message JAGDataSlice {
    repeated JAG_Data pieces = 1;
  }
  repeated JAGDataSlice independent = 97;
  repeated JAGDataSlice dependent = 98;

  int32 max_files_to_load = 1000;

  // for jag_conduit_hdf5
  string scalar_keys = 1004;
  string input_keys = 1005;
  string image_views = 1006;
  string image_channels = 1007;
  //------------------  end of only for jag_conduit  -----------------------

  int32 num_labels = 99; //for imagenet and synthetic
  int64 num_samples = 100; //only for synthetic
  string synth_dimensions = 101; //only for synthetic
  string synth_response_dimensions = 115; //only for synthetic
  //csv attributes
  string separator = 102;
  int32 skip_cols = 103;
  int32 skip_rows = 104;
  bool has_header = 105;
  int32 label_col = 106;
  int32 response_col = 107;
  bool disable_labels = 108;
  bool disable_responses = 109;
  string format = 110; // numpy, csv
  string data_file_pattern = 111;
  int64 num_neighbors = 112; // pilot2_molecular_reader
  int64 max_neighborhood = 113; // pilot2_molecular_reader
  int32 num_image_srcs = 114; // data_reader_multi_images
  float scaling_factor_int16 = 116; // for numpy_npz_reader with int16 data
  bool merge_skip_overlapped = 117; // for data_reader_merge_samples

  //------------- start of only for partitioned data sets ------------------
  bool is_partitioned = 300;
  double partition_overlap = 301;
  int32 partition_mode = 302;
       // 1 - share a portion of your data with two neighbors;
       // 2 - there's a set of overlap indices that are common to all models
  //------------- end of only for partitioned data sets ------------------
}

message ImagePreprocessor {
  string name = 1;
  bool disable = 2;
  int32 raw_width = 3;
  int32 raw_height = 4;
  int32 raw_num_channels = 5;

  message Cropper {
    string name = 1;
    bool disable = 2;
    bool crop_randomly = 3;
    uint32 crop_width = 4;
    uint32 crop_height = 5;
    int32 resized_width = 6;
    int32 resized_height = 7;
    bool adaptive_interpolation = 8;
  }

  message Resizer {
    string name = 1;
    bool disable = 2;
    int32 resized_width = 3;
    int32 resized_height = 4;
    bool adaptive_interpolation = 5;
  }

  message Augmenter {
    string name = 1;
    bool disable = 2;
    bool horizontal_flip = 3;
    bool vertical_flip = 4;
    double rotation = 5;
    double horizontal_shift = 6;
    double vertical_shift = 7;
    double shear_range = 8;
  }

  message Decolorizer {
    string name = 1;
    bool disable = 2;
    bool pick_1ch = 3;
  }

  message Colorizer {
    string name = 1;
    bool disable = 2;
  }

  message Normalizer {
    string name = 1;
    bool disable = 2;
    bool scale = 3;
    bool subtract_mean = 4;
    bool unit_variance = 5;
    bool z_score = 6;
  }

  message Subtractor {
    string name = 1;
    bool disable = 2;
    string image_to_sub = 3;
    string image_to_div = 4;
    repeated float channel_mean = 5 [packed = true];
    repeated float channel_stddev = 6 [packed = true];
  }

  message PatchExtractor {
    string name = 1;
    bool disable = 2;
    uint32 patch_width = 3;
    uint32 patch_height = 4;
    uint32 patch_gap = 5; // gap between patches
    uint32 patch_jitter = 6; // max jittering amount for patch positions
    uint32 centering_mode = 7; // center patch positioning mode
    uint32 ca_correction_mode = 8; // chromatic abberation correction mode
  }

  message Noiser {
    string name = 1;
    bool disable = 2;
    float factor = 3;
  }

  Cropper cropper = 6;
  Resizer resizer = 7;
  Augmenter augmenter = 8;
  Decolorizer decolorizer = 9;
  Colorizer colorizer = 10;
  Subtractor subtractor = 11;
  Normalizer normalizer = 12;
  Noiser noiser = 13;
  PatchExtractor patch_extractor = 14;

  int32 early_normalization = 33; // for data_reader_jag only
}

// TODO: wrap El::Mat based normalization into a generic preprocessor
message GenericPreprocessor {
  string name = 1;
  bool disable = 2;

  message Normalizer {
    string name = 1;
    bool disable = 2;
    bool scale = 3;
    bool subtract_mean = 4;
    bool unit_variance = 5;
    bool z_score = 6;
  }

  Normalizer normalizer = 3;
}

//========================================================================
// Model
//========================================================================

message Model {
  string type = 1;
  string name = 3;
  ObjectiveFunction objective_function = 2;
  repeated Metric metric = 5;
  string data_layout = 6;
  bool shareable_training_data_reader = 42; // Can the data reader be shared across multiple models (e.g. GAN)
  bool shareable_testing_data_reader = 43; // Can the data reader be shared across multiple models (e.g. GAN)
  bool shareable_validation_data_reader = 44; // Can the data reader be shared across multiple models (e.g. GAN)

  int64 mini_batch_size = 12;
  int64 num_epochs = 4;
  int64 super_steps = 121; //multiple steps/epochs currently use in GAN
  int64 num_batches = 122; //multiple batches/sub epoch
  int64 block_size = 50;
  int64 procs_per_trainer = 51;
  int64 num_gpus = 53; //has no effect
  int64 evaluation_frequency = 54;
  int64 num_parallel_readers = 100;
  bool  serialize_background_io = 101;

  bool disable_cuda = 8;

  repeated Layer layer = 10;

  repeated Weights weights = 11;

  repeated Callback callback = 20;

  int64 random_seed = 30;
  // If true, models will have their model rank mixed into their random seed.
  bool random_init_models_differently = 31;

}

//========================================================================
// Objective function
//========================================================================

message ObjectiveFunction {
  repeated MeanSquaredError mean_squared_error = 10;
  repeated MeanAbsoluteDeviation mean_absolute_deviation = 11;
  repeated MeanAbsoluteError mean_absolute_error = 24;
  repeated CrossEntropy cross_entropy = 12;
  repeated BinaryCrossEntropy binary_cross_entropy = 13;
  repeated CrossEntropyWithUncertainty cross_entropy_with_uncertainty = 14;
  repeated GeomNegLogLike geom_negloglike = 15;
  repeated PoissonNegLogLike poisson_negloglike = 16;
  repeated PolyaNegLogLike polya_negloglike = 17;
  repeated L1WeightRegularization l1_weight_regularization = 20;
  repeated L2WeightRegularization l2_weight_regularization = 21;
  repeated GroupLassoWeightRegularization group_lasso_weight_regularization = 22;
  repeated LayerTerm layer_term = 25;
}

message MeanSquaredError {
  double scale_factor = 1;
}

message MeanAbsoluteDeviation {
  double scale_factor = 1;
}

message MeanAbsoluteError {
  double scale_factor = 1;
}

message CrossEntropy {
  double scale_factor = 1;
}


message BinaryCrossEntropy {
  double scale_factor = 1;
}

message CrossEntropyWithUncertainty {
  double scale_factor = 1;
}

message GeomNegLogLike {
  double scale_factor = 1;
}

message PoissonNegLogLike {
  double scale_factor = 1;
}

message PolyaNegLogLike {
  double scale_factor = 1;
}

message L1WeightRegularization {
  double scale_factor = 1;
}

message L2WeightRegularization {
  double scale_factor = 1;
  string weights = 2;   // If empty, L2 regularization is applied to all weights
}

message GroupLassoWeightRegularization {
  double scale_factor = 1;
}

message LayerTerm {
  double scale_factor = 1;
  string layer = 2;
}

//========================================================================
// Metrics
//========================================================================

message Metric {
  LayerMetric layer_metric = 11;
}

message LayerMetric {
  string layer = 1;
  string name = 2;
  string unit = 3;
}

//========================================================================
// Optimizers
//========================================================================
message Optimizer {
  // An Optimizer should contain exactly one of the following
  // (this may or may not be properly checked for in proto_common.cpp)
  Adagrad adagrad = 1;
  Rmsprop rmsprop = 2;
  Adam adam = 3;
  HypergradientAdam hypergradient_adam = 4;
  Sgd sgd = 5;
}

message Adagrad {
  double learn_rate = 1;
  double eps = 2;  //default: 1e-8
}

message Adam {
  double learn_rate = 1;
  double beta1 = 6;    //default: 0.9
  double beta2 = 7;    //default: 0.99
  double eps = 8;      //default: 1e-8
}

message HypergradientAdam {
  double init_learning_rate = 1;
  double hyper_learning_rate = 2; //default: 1e-7
  double beta1 = 6;    //default: 0.9
  double beta2 = 7;    //default: 0.99
  double eps = 8;      //default: 1e-8
}

message Rmsprop {
  double learn_rate = 1;
  double decay_rate = 2;
  double eps = 3; //default: 1e-8
}

message Sgd {
  double learn_rate = 1;
  double momentum = 2;     //default: 0
  double decay_rate = 3;   //default: 0
  bool nesterov = 4;       //default: false
}


//========================================================================
// Callbacks
//========================================================================
message Callback {
   // a Callback should contain exactly one of the following
   CallbackPrint print = 1;
   CallbackTimer timer = 2;
   CallbackSummary summary = 3;
   CallbackDumpWeights dump_weights = 4;
   CallbackDumpOutputs dump_outputs = 5;
   CallbackDumpErrorSignals dump_error_signals = 35;
   CallbackDumpGradients dump_gradients = 6;
   CallbackDumpMBIndices dump_mb_indices = 7;
   CallbackDispIOStats disp_io_stats = 8;
   CallbackImComm imcomm = 9;
   CallbackSaveImages save_images = 10;
   CallbackDebug debug = 11;
   CallbackAdaptiveLearningRate adaptive_learning_rate = 12;
   CallbackStepLearningRate step_learning_rate = 13;
   CallbackCustomLearningRate custom_learning_rate = 14;
   CallbackCheckSmall check_small = 15;
   CallbackCheckNaN check_nan = 16;
   CallbackCheckDataset check_dataset = 17;
   CallbackHang hang = 18;
   CallbackDropFixedLearningRate drop_fixed_learning_rate = 19;
   CallbackLinearGrowthLearningRate linear_growth_learning_rate = 20;
   CallbackProfiler profiler = 21;
   CallbackStepMinibatch step_minibatch = 22;
   CallbackCheckGradients check_gradients = 23;
   CallbackLTFB ltfb = 24;
   CallbackDebugIO debug_io = 25;
   CallbackMinibatchSchedule minibatch_schedule = 26;
   CallbackOptimizerwiseAdaptiveLearningRate optimizerwise_adaptive_learning_rate = 27;
   CallbackCheckpoint checkpoint = 28;
   CallbackSaveModel save_model = 29;
   CallbackPolyLearningRate poly_learning_rate = 30;
   CallbackReplaceWeights replace_weights = 31;
   CallbackGPUMemoryUsage gpu_memory_usage = 32;
   CallbackSyncLayers sync_layers = 33;
   CallbackSyncSelected sync_selected = 34;
   CallbackConfusionMatrix confusion_matrix = 36;
   CallbackCheckMetric check_metric = 37;
   CallbackPerturbAdam perturb_adam = 38;
}

message CallbackLTFB {
  int64 batch_interval = 1;
  string metric = 2;
  string weights = 3;       // default: all weights
  bool low_score_wins = 4;
  string communication_algorithm = 5;   // default: "sendrecv_weights"
}

message CallbackStepLearningRate {
  string weights = 1; //default: all weights
  int64 step = 2;
  double amt = 3;
}

message CallbackCustomLearningRate {
  //don't know how to support this, since it takes an std::function as an argument
}

message CallbackAdaptiveLearningRate {
  string weights = 1; //default: all weights
  int64 patience = 2;
  double amt = 3;
}

message CallbackSaveImages {
  string layers       = 1; // Layer outputs to save as images
  string image_format = 2; // Image format (e.g. jpg, png, pgm)
  string image_prefix = 3; // Prefix for saved image files
}

message CallbackPrint {
  int64 interval = 1; //default in lbann_callback_print.hpp is 1
}

message CallbackProfiler {
  bool sync = 1;
}

message CallbackTimer {
}

message CallbackSummary {
  string dir = 1; //directory for the lbann_summary
  int64 batch_interval = 2; //default in lbann_callback_summary.hpp is 1
  int64 mat_interval = 3; //default in lbann_callback_summary.hpp is 25
}

message CallbackDumpWeights {
  string basename = 1;
}

message CallbackDumpOutputs {
  string layers = 1;          // Default: all layers
  string execution_modes = 2; // Default: all modes
  int64 batch_interval = 3;   // Frequency for output dumping (default: all steps)
  string directory = 4;       // Directory for output files
  string format = 5;          // Options: csv, tsv, npy, npz (default: csv)
}

message CallbackDumpErrorSignals {
  string basename = 1;
}

message CallbackDumpGradients {
  string basename = 1;
  int64 interval = 2;
}

message CallbackDumpMBIndices {
  string basename = 1;
  int64 interval = 2;
}

message CallbackDispIOStats {
  string layers = 1; //e.g: "2 4 5"; use "10000" to apply to all layers
}

message CallbackImComm {
  string intertrainer_comm_method = 1;
  bool all_optimizers = 2;
}

message CallbackDebug {
  string phase = 1; //should be called "modes"
}

message CallbackDebugIO {
  string phase = 1;
  int32 lvl = 2;
}

message CallbackCheckSmall {
}

message CallbackCheckNaN {
}

message CallbackCheckDataset {
}

message CallbackHang {
  int64 rank = 1;
}

message CallbackDropFixedLearningRate {
  string weights = 1;
  repeated int64 drop_epoch = 2;
  double amt = 3;
}

message CallbackLinearGrowthLearningRate {
  string weights = 1;
  double target = 2;
  int64 num_epochs = 3;
  int64 delay = 4;
}

message CallbackPolyLearningRate {
  string weights = 1;
  double power = 2;
  uint64 num_epochs = 3;
  uint64 max_iter = 4;
  double end_lr = 5;
}

message CallbackStepMinibatch {
  int64 starting_mbsize = 1;
  int64 step = 2;
  int64 ramp_time = 3;
}

message MinibatchScheduleStep {
  int64 epoch = 1;
  int64 mbsize = 2;
  double lr = 3;
  int64 ramp_time = 4;
}

message CallbackOptimizerwiseAdaptiveLearningRate {
  string weights = 1;
  double scale = 2;
}

message CallbackMinibatchSchedule {
  int64 starting_mbsize = 1;
  repeated MinibatchScheduleStep step = 2;
}

message CallbackCheckGradients {
  double step_size = 1;
  bool verbose = 2;
  bool error_on_failure = 3; // Throw error if gradient check fails
}

message CallbackCheckMetric {
  string metric = 1;
  double lower_bound = 2;
  double upper_bound = 3;
  bool error_on_failure = 4;  // Throw error if metric check fails
  string execution_modes = 5; // Default: all modes
}

message CallbackCheckpoint {
  string checkpoint_dir = 1;
  int64 checkpoint_epochs = 2;
  int64 checkpoint_steps = 3;
  double checkpoint_secs = 4;
  string per_rank_dir = 5;
  int64 ckpt_dist_epochs = 6;
  int64 ckpt_dist_steps = 7;
}


message CallbackSaveModel {
  string dir = 1;
  string extension = 2;
  bool disable_save_after_training = 3;
}

message CallbackReplaceWeights {
  string source_layers = 1; //set of layers to copy weights from
  string destination_layers = 2;  //set of layers to copy weights to
  int64 batch_interval = 3;
}
message CallbackGPUMemoryUsage {
}

message CallbackSyncLayers {
  bool sync_gpus = 1;
  bool sync_mpi = 2;
  bool only_input = 3;
}

message CallbackSyncSelected {
  message LayerToSync {
    enum PropDirection {
      Both = 0;
      Forward = 1;
      Backward = 2;
    }
    string name = 1; // name of the layer to synchronize
    PropDirection prop = 2; // propagation setep to synchronize
  }

  message CudaProfilerSetup {
    enum OutputMode {
      KeyValuePair = 0;
      CSV = 1;
    }
    bool no_init = 1;
    string config_file = 2;
    string output_dir = 3;
    OutputMode output_mode = 4;
  }

  bool async_gpus = 1;
  bool async_mpi = 2;
  repeated LayerToSync layer_to_sync = 3;
  CudaProfilerSetup cuda_profiler_setup = 4;
}

message CallbackConfusionMatrix {
  string prediction = 1; // Prediction layer
  string label = 2;      // Label layer
  string prefix = 3;     // Prefix for output files
}

message CallbackPerturbAdam {
  float learning_rate_factor = 1;   // Learning rate perturbation (in log space)
  float beta1_factor = 2;           // beta1 perturbation (in log space)
  float beta2_factor = 3;           // beta2 perturbation (in log space)
  float eps_factor = 4;             // eps perturbation (in log space)
  bool perturb_during_training = 5; // Whether to periodically perturb during training
  int64 batch_interval = 6;         // Frequency of perturbation if perturb_during_training is true
  string weights = 7;               // Weights with Adam optimizer
}

//========================================================================
// Weights
//========================================================================

message Weights {

  string name = 1;
  Optimizer optimizer = 2;

  ConstantInitializer constant_initializer = 20;
  ValueInitializer value_initializer = 21;
  UniformInitializer uniform_initializer = 22;
  NormalInitializer normal_initializer = 23;
  GlorotNormalInitializer glorot_normal_initializer = 24;
  GlorotUniformInitializer glorot_uniform_initializer = 25;
  HeNormalInitializer he_normal_initializer = 26;
  HeUniformInitializer he_uniform_initializer = 27;
  LeCunNormalInitializer lecun_normal_initializer = 28;
  LeCunUniformInitializer lecun_uniform_initializer = 29;

}

// Weight initializers
message ConstantInitializer {
  double value = 1;
}
message ValueInitializer {
  string values = 1;
}
message UniformInitializer {
  double min = 1;
  double max = 2;
}
message NormalInitializer {
  double mean = 1;
  double standard_deviation = 2;
}
message GlorotNormalInitializer {}
message GlorotUniformInitializer {}
message HeNormalInitializer {}
message HeUniformInitializer {}
message LeCunNormalInitializer {}
message LeCunUniformInitializer {}

//note: I'd like to put this enum inside of Layer, but if I do the enum values
//      become, e.g, Layer_Imcomm_EXCLUDE, which is just ugly
enum Imcomm {
  DEFAULT = 0; //add Layer to Imcomm callback if all_learning_layers = true in
               //the CallbackImComm
  EXCLUDE = 1; //*do not* add Layer to Imcomm callback if all_learning_layers = true in
               //the CallbackImComm
  INCLUDE = 2;  //add Layer to Imcomm callback regardless of whether all_learning_layers
                //in the CallbackImComm is set to true or false
}

// Weight data for exporting
message WeightsShape {
  repeated int64 dim = 1 [packed = true];
}
message WeightsData {
  WeightsShape shape = 5;
  string name = 1;
  int64 height = 2;
  int64 width = 3;
  //@todo assume float above, add other datatype
  repeated float data = 4 [packed=true];

  Imcomm imcomm = 55;
}

//========================================================================
// MotifDefinitions
//========================================================================

message MotifDefinitions {
  repeated Motif motif = 1;
}

message Motif {
  string name = 1;
  repeated Layer layer = 2;
}

//========================================================================
// Layers
//========================================================================

message Layer {
   string name = 50;
   string parents = 151;
   string children = 152;
   string data_layout = 52;
   string device_allocation = 55;
   string weights = 54;
   bool num_neurons_from_data_reader = 53;
   bool freeze = 5;
   string hint_layer = 56;

   repeated WeightsData weights_data = 153;
   string top = 154;
   string bottom = 155;
   string type = 156;

   // a Layer should contain exactly one of the following
   // (this may or may not be properly checked for in proto_common.cpp)
   //
   // @todo: this should be done better using oneof:
   //   oneof a_layer {
   //       Reshape reshape = 306
   //       Pooling pooling = 12;
   //       ...
   //   }
   //
   //

   // motif layer
   MotifLayer motif_layer = 4;

   // Input layers
   Input input = 2;

   // Transform layers
   Reshape reshape = 306;
   Pooling pooling = 12;
   Concatenation concatenation = 300;
   Slice slice = 301;
   Split split = 302;
   Sum sum = 303;
   WeightedSum weighted_sum = 323;
   Unpooling unpooling = 304;
   Hadamard hadamard = 308;
   Constant constant = 309;
   Zero zero = 315;
   Reduction reduction = 310;
   Evaluation evaluation = 311;
   Gaussian gaussian = 312;
   Bernoulli bernoulli = 313;
   Uniform uniform = 314;
   Crop crop = 316;
   CategoricalRandom categorical_random = 317;
   DiscreteRandom discrete_random = 318;
   Dummy dummy = 319;
   StopGradient stop_gradient = 320;
   InTopK in_top_k = 324;
   Sort sort = 325;
   WeightsLayer weights_layer = 326;
   Tessellate tessellate = 327;

   // Learning layers
   FullyConnected fully_connected = 11;
   Convolution convolution = 13;
   Deconvolution deconvolution = 305;

   // Loss layers
   CategoricalAccuracy categorical_accuracy = 60;
   CrossEntropy cross_entropy = 61;
   MeanSquaredError mean_squared_error = 62;
   MeanAbsoluteError mean_absolute_error = 63;
   TopKCategoricalAccuracy top_k_categorical_accuracy = 64;
   L2Norm2 l2_norm2 = 65;
   L1Norm l1_norm = 66;
   BinaryCrossEntropy binary_cross_entropy = 67;
   SigmoidBinaryCrossEntropy sigmoid_binary_cross_entropy = 68;
   BooleanAccuracy boolean_accuracy = 69;
   BooleanFalseNegative boolean_false_negative = 70;
   BooleanFalsePositive boolean_false_positive = 71;

   // Math layers
   LogicalNot logical_not = 401;
   Abs abs = 402;
   Negative negative = 403;
   Sign sign = 404;
   Round round = 405;
   Ceil ceil = 406;
   Floor floor = 407;
   Reciprocal reciprocal = 408;
   Square square = 409;
   Sqrt sqrt = 410;
   Rsqrt rsqrt = 411;
   SafeReciprocal safe_reciprocal = 412;
   Exp exp = 413;
   Expm1 expm1 = 414;
   Log log = 415;
   Log1p log1p = 416;
   Cos cos = 417;
   Sin sin = 418;
   Tan tan = 419;
   Acos acos = 420;
   Asin asin = 421;
   Atan atan = 422;
   Cosh cosh = 423;
   Sinh sinh = 424;
   Tanh tanh = 425;
   Acosh acosh = 426;
   Asinh asinh = 427;
   Atanh atanh = 428;
   Add add = 450;
   Subtract subtract = 451;
   Multiply multiply = 452;
   Divide divide = 453;
   Mod mod = 454;
   Pow pow = 455;
   SafeDivide safe_divide = 456;
   SquaredDifference squared_difference = 457;
   Max max = 458;
   Min min = 459;
   Equal equal = 460;
   NotEqual not_equal = 461;
   Less less = 462;
   LessEqual less_equal = 463;
   Greater greater = 464;
   GreaterEqual greater_equal = 465;
   LogicalAnd logical_and = 466;
   LogicalOr logical_or = 467;
   LogicalXor logical_xor = 468;
   Clamp clamp = 469;

   // Target layers
   Target target = 18;
   TargetReconstruction reconstruction = 22;

   // Regularization layers
   BatchNormalization batch_normalization = 19;
   LocalResponseNormalization local_response_normalization = 20;
   Dropout dropout = 21;
   SeluDropout selu_dropout = 229;

   // Activation layers
   Elu elu = 200;
   Identity identity = 201;
   LeakyRelu leaky_relu = 202;
   LogSigmoid log_sigmoid = 203;
   LogSoftmax log_softmax = 204;
   Relu relu = 205;
   Selu selu = 206;
   Sigmoid sigmoid = 207;
   Softmax softmax = 208;
   Softplus softplus = 209;
   Softsign softsign = 210;

   // Image layers
   BilinearResize bilinear_resize = 500;

   // Miscellaneous layers
   Covariance covariance = 600;
   Variance variance = 601;
   ChannelwiseMean channelwise_mean = 602;

}
///////////////////////
// MotifLayer //
///////////////////////
message MotifLayer {
  string motif_id = 1;
  repeated string variable = 2;
}

///////////////////////
// Math layers       //
///////////////////////
message LogicalNot {}
message Abs {}
message Negative {}
message Sign {}
message Round {}
message Ceil {}
message Floor {}
message Reciprocal {}
message Square {}
message Sqrt {}
message Rsqrt {}
message SafeReciprocal {}
message Exp {}
message Expm1 {}
message Log {}
message Log1p {}
message Cos {}
message Sin {}
message Tan {}
message Acos {}
message Asin {}
message Atan {}
message Cosh {}
message Sinh {}
message Tanh {}
message Acosh {}
message Asinh {}
message Atanh {}
message Add {}
message Subtract {}
message Multiply {}
message Divide {}
message Mod {}
message Pow {}
message SafeDivide {}
message SquaredDifference {}
message Max {}
message Min {}
message Equal {}
message NotEqual {}
message Less {}
message LessEqual {}
message Greater {}
message GreaterEqual {}
message LogicalAnd {}
message LogicalOr {}
message LogicalXor {}
message Clamp {
  double min = 1;
  double max = 2;
}

///////////////////////
// Activation layers //
///////////////////////
message Elu {
  double alpha = 1; //default: 1.0; should be >= 0
}
message Identity {}
message LeakyRelu {
  double negative_slope = 1; //default: 0.01
}
message LogSigmoid {}
message LogSoftmax {}
message Relu {}
message Selu {}
message Sigmoid {}
message Softmax {}
message Softplus {}
message Softsign {}

///////////////////////
// Loss layers //
///////////////////////
message L2Norm2 {}
message L1Norm {}
message SigmoidBinaryCrossEntropy {}
message CategoricalAccuracy {}
message TopKCategoricalAccuracy {
  int64 k = 1;
}
message BooleanAccuracy {}
message BooleanFalseNegative {}
message BooleanFalsePositive {}

///////////////////////////
// Regularization layers //
///////////////////////////
message BatchNormalization {
  double decay = 1;          //default: 0.9
  double scale_init = 2;     //default: 1.0
  double bias_init = 3;      //default: 0.0
  double epsilon = 4;        //default: 1e-5
  string stats_aggregation = 5; // default: local
}

message SeluDropout {
  double keep_prob = 2; //default: 0.95
  double alpha = 3;     //default: 1.6732632423543772848170429916717
  double scale = 4;     //default: 1.0507009873554804934193349852946
}

message LocalResponseNormalization {
  int64 window_width = 4;
  double lrn_alpha = 5;
  double lrn_beta = 6;
  double lrn_k = 7;
}

message Dropout {
  double keep_prob = 2;  //default: 0.5
}

//////////////////
// Input layers //
//////////////////
message Input {
  bool data_set_per_model = 1;  //default: false
  string io_buffer = 2;
  string target_mode = 3;
}

/// @todo Remove when possible
message RepeatedInput {
  bool data_set_per_model = 1;  //default: false
  int64 num_steps = 2;
  string target_mode = 3;
}

//////////////////////
// Transform layers //
//////////////////////
message Reshape {
  int64 num_dims = 1; //DEPRECATED
  string dims = 2; //should be space-separated list of ints, e.g, "2 6 7"
}

message Pooling {
  int64 num_dims = 1;

  bool has_vectors = 2;

  //these are used if has_vectors = true
  string pool_dims = 4; //should be space-separated list, e.g, "2 2 3"
  string pool_pads = 5; //should be space-separated list, e.g, "2 2 3"
  string pool_strides = 6; //should be space-separated list, e.g, "2 2 3"

  //these are used if has_vectors = false
  int64 pool_dims_i = 10;
  int64 pool_pads_i = 11;
  int64 pool_strides_i = 12;

  //pool_mode should be one of: max, average, average_no_pad
  //see: lbann/include/lbann/lbann_base.hpp
  string pool_mode = 7;
}

message Unpooling {
  int64 num_dims = 1;
  string pooling_layer = 13; //should be name of the pooling layer
}


message Concatenation {
  string parents = 1; //TODO: this doesn't do anything and should be removed
  int64 concatenation_axis = 2;
}

message Slice {
  int64 slice_axis = 2;
  string slice_points = 3; //should be space-separated list of ints, e.g, "2 6 7"
  //the following is for jag_conduit_hdf5;
  string get_slice_points_from_reader = 4;
  bool get_slice_points_from_reader_bool = 5;
}

message Split {
}

message Sum {
}

message WeightedSum {
  string scaling_factors = 1; //should be a space-separated list of doubles, e.g. "1.0 2.0 -1.0"
}

message Hadamard {
}

message Constant {
  double value=1;
  string num_neurons=2;
}


message Zero {
  bool first_half=1; //default: true
  bool second_half=2; //default: true
}

message Reduction {
  string mode=1; //"sum" or "average"
}

message Evaluation {
}

message Gaussian {
  double mean = 1;
  double stdev = 2;
  string neuron_dims = 3;
}

message Bernoulli {
  double prob = 1;
  string neuron_dims = 2;
}

message Uniform {
  double min = 1;
  double max = 2;
  string neuron_dims = 3;
}


message Crop {
  string dims = 3;
}

message CategoricalRandom {
}

message DiscreteRandom {
  string values = 1;
  string dims = 2;
}

message Dummy {
}

message StopGradient {
}

message InTopK {
  int64 k = 1;
}

message Sort {
  bool descending = 1;
}

message WeightsLayer {
  string dims = 1;
}

message Tessellate {
  string dims = 1;
}

/////////////////////
// Learning layers //
/////////////////////
message FullyConnected {
  int64 num_neurons = 1;
  string weight_initialization = 2;    //DEPRECATED
  bool has_bias = 3;                   //default: true
  double bias_initial_value = 4;       //default: 0
  double l2_regularization_factor = 5; //default: 0
  double group_lasso_regularization_factor = 6; //default: 0
  bool transpose = 7;
  bool num_neurons_is_num_labels = 8;

  bool get_input_dimension_from_reader = 9;
  bool get_image_and_scalar_dimension_from_reader = 10;
  bool get_image_dimension_from_reader = 11;
  bool get_scalar_dimension_from_reader = 12;
  repeated uint32 get_num_neurons_of_slice_from_reader = 13;
  string get_slice_points_from_reader = 14;
}

message Convolution {
  int64 num_dims = 1;
  int64 num_output_channels = 4;
  int64 num_groups = 3;

  bool has_vectors = 2;

  // these are used if has_vector = true
  string conv_dims = 5; //should be space-separated list, e.g, "2 2 3"
  string conv_pads = 6;  //should be space-separated list, e.g, "2 2 3"
  string conv_strides = 7; //should be space-separated list, e.g, "2 2 3"
  string conv_dilations = 8;  //should be space-separated list, e.g. "2 3 3"

  // these are used if has_vector = false
  int64 conv_dims_i = 50;
  int64 conv_pads_i = 60;
  int64 conv_strides_i = 70;
  int64 conv_dilations_i = 80;

  string weight_initialization = 9;     //DEPRECATED
  bool has_bias = 10;                   //default: true
  double bias_initial_value = 11;       //default: 0
  double l2_regularization_factor = 12; //default: 0
}

message Deconvolution {
  int64 num_dims = 1;
  int64 num_output_channels = 4;
  int64 num_groups = 3;

  bool has_vectors = 2;

  // these are used if has_vector = true
  string conv_dims = 5; //should be space-separated list, e.g, "2 2 3"
  string conv_pads = 6;  //should be space-separated list, e.g, "2 2 3"
  string conv_strides = 7; //should be space-separated list, e.g, "2 2 3"
  string conv_dilations = 8;  //should be space-separated list, e.g. "2 3 3"

  // these are used if has_vector = false
  int64 conv_dims_i = 50;
  int64 conv_pads_i = 60;
  int64 conv_strides_i = 70;
  int64 conv_dilations_i = 80;

  string weight_initialization = 9;     //DEPRECATED
  bool has_bias = 10;                   //default: true
  double bias_initial_value = 11;       //default: 0
  double l2_regularization_factor = 12; //default: 0
}

///////////////////
// Target layers //
///////////////////
message Target {
  string paired_input_layer = 1;
  bool shared_data_reader = 2;
  bool for_regression = 3; //default: false
  string io_buffer = 4;
}

message TargetReconstruction {
}

//////////////////
// Image layers //
//////////////////
message BilinearResize {
  int64 height = 1;
  int64 width = 2;
}

//////////////////////////
// Miscellaneous layers //
//////////////////////////
message Covariance {
  bool biased = 1; //Whether to use a biased covariance estimate
}
message Variance {
  bool biased = 1; //Whether to use a biased variance estimate
}
message ChannelwiseMean {}
